{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.397561\n",
      "Epoch: 2 \tTraining Loss: 0.130699\n",
      "Epoch: 3 \tTraining Loss: 0.088399\n",
      "Epoch: 4 \tTraining Loss: 0.071318\n",
      "Epoch: 5 \tTraining Loss: 0.058989\n",
      "Epoch: 6 \tTraining Loss: 0.050542\n",
      "Epoch: 7 \tTraining Loss: 0.044438\n",
      "Epoch: 8 \tTraining Loss: 0.038261\n",
      "Epoch: 9 \tTraining Loss: 0.034616\n",
      "Epoch: 10 \tTraining Loss: 0.031767\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(73)\n",
    "\n",
    "train_data = datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_data = datasets.MNIST('data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self, hidden=64, output=10):\n",
    "        super(ConvNet, self).__init__()        \n",
    "        self.conv1 = torch.nn.Conv2d(1, 4, kernel_size=7, padding=0, stride=3)\n",
    "        self.fc1 = torch.nn.Linear(256, hidden)\n",
    "        self.fc2 = torch.nn.Linear(hidden, output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        # the model uses the square activation function\n",
    "        x = x * x\n",
    "        # flattening while keeping the batch axis\n",
    "        x = x.view(-1, 256)\n",
    "        x = self.fc1(x)\n",
    "        x = x * x\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, n_epochs=10):\n",
    "    # model in training mode\n",
    "    model.train()\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "\n",
    "        train_loss = 0.0\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # calculate average losses\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))\n",
    "    \n",
    "    # model in evaluation mode\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "model = ConvNet()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "model = train(model, train_loader, criterion, optimizer, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.077243\n",
      "\n",
      "Test Accuracy of 0: 99% (971/980)\n",
      "Test Accuracy of 1: 99% (1130/1135)\n",
      "Test Accuracy of 2: 98% (1016/1032)\n",
      "Test Accuracy of 3: 99% (1003/1010)\n",
      "Test Accuracy of 4: 98% (969/982)\n",
      "Test Accuracy of 5: 98% (880/892)\n",
      "Test Accuracy of 6: 98% (947/958)\n",
      "Test Accuracy of 7: 98% (1011/1028)\n",
      "Test Accuracy of 8: 96% (940/974)\n",
      "Test Accuracy of 9: 96% (973/1009)\n",
      "\n",
      "Test Accuracy (Overall): 98% (9840/10000)\n"
     ]
    }
   ],
   "source": [
    "def test(model, test_loader, criterion):\n",
    "    # initialize lists to monitor test loss and accuracy\n",
    "    test_loss = 0.0\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "\n",
    "    # model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    for data, target in test_loader:\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss.item()\n",
    "        # convert output probabilities to predicted class\n",
    "        _, pred = torch.max(output, 1)\n",
    "        # compare predictions to true label\n",
    "        correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
    "        # calculate test accuracy for each object class\n",
    "        for i in range(len(target)):\n",
    "            label = target.data[i]\n",
    "            class_correct[label] += correct[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "    # calculate and print avg test loss\n",
    "    test_loss = test_loss/len(test_loader)\n",
    "    print(f'Test Loss: {test_loss:.6f}\\n')\n",
    "\n",
    "    for label in range(10):\n",
    "        print(\n",
    "            f'Test Accuracy of {label}: {int(100 * class_correct[label] / class_total[label])}% '\n",
    "            f'({int(np.sum(class_correct[label]))}/{int(np.sum(class_total[label]))})'\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        f'\\nTest Accuracy (Overall): {int(100 * np.sum(class_correct) / np.sum(class_total))}% ' \n",
    "        f'({int(np.sum(class_correct))}/{int(np.sum(class_total))})'\n",
    "    )\n",
    "    \n",
    "test(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "It's a PyTorch-like model using operations implemented in TenSEAL.\n",
    "    - .mm() method is doing the vector-matrix multiplication explained above.\n",
    "    - you can use + operator to add a plain vector as a bias.\n",
    "    - .conv2d_im2col() method is doing a single convolution operation.\n",
    "    - .square_() just square the encrypted vector inplace.\n",
    "\"\"\"\n",
    "\n",
    "import tenseal as ts\n",
    "\n",
    "\n",
    "class EncConvNet:\n",
    "    def __init__(self, torch_nn):\n",
    "        self.conv1_weight = torch_nn.conv1.weight.data.view(\n",
    "            torch_nn.conv1.out_channels, torch_nn.conv1.kernel_size[0],\n",
    "            torch_nn.conv1.kernel_size[1]\n",
    "        ).tolist()\n",
    "        self.conv1_bias = torch_nn.conv1.bias.data.tolist()\n",
    "        \n",
    "        self.fc1_weight = torch_nn.fc1.weight.T.data.tolist()\n",
    "        self.fc1_bias = torch_nn.fc1.bias.data.tolist()\n",
    "        \n",
    "        self.fc2_weight = torch_nn.fc2.weight.T.data.tolist()\n",
    "        self.fc2_bias = torch_nn.fc2.bias.data.tolist()\n",
    "        \n",
    "        \n",
    "    def forward(self, enc_x, windows_nb):\n",
    "        # conv layer\n",
    "        enc_channels = []\n",
    "        for kernel, bias in zip(self.conv1_weight, self.conv1_bias):\n",
    "            y = enc_x.conv2d_im2col(kernel, windows_nb) + bias\n",
    "            enc_channels.append(y)\n",
    "        # pack all channels into a single flattened vector\n",
    "        enc_x = ts.CKKSVector.pack_vectors(enc_channels)\n",
    "        # square activation\n",
    "        enc_x.square_()\n",
    "        # fc1 layer\n",
    "        enc_x = enc_x.mm(self.fc1_weight) + self.fc1_bias\n",
    "        # square activation\n",
    "        enc_x.square_()\n",
    "        # fc2 layer\n",
    "        enc_x = enc_x.mm(self.fc2_weight) + self.fc2_bias\n",
    "        return enc_x\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)\n",
    "\n",
    "    \n",
    "def enc_test(context, model, test_loader, criterion, kernel_shape, stride):\n",
    "    # initialize lists to monitor test loss and accuracy\n",
    "    test_loss = 0.0\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "\n",
    "    for data, target in test_loader:\n",
    "        # Encoding and encryption\n",
    "        x_enc, windows_nb = ts.im2col_encoding(\n",
    "            context, data.view(28, 28).tolist(), kernel_shape[0],\n",
    "            kernel_shape[1], stride\n",
    "        )\n",
    "        # Encrypted evaluation\n",
    "        enc_output = model(x_enc, windows_nb)\n",
    "        # Decryption of result\n",
    "        output = enc_output.decrypt()\n",
    "        output = torch.tensor(output).view(1, -1)\n",
    "\n",
    "        # compute loss\n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        # convert output probabilities to predicted class\n",
    "        _, pred = torch.max(output, 1)\n",
    "        # compare predictions to true label\n",
    "        correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
    "        # calculate test accuracy for each object class\n",
    "        label = target.data[0]\n",
    "        class_correct[label] += correct.item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "        print('aaaaaaaaaaa')\n",
    "\n",
    "\n",
    "    # calculate and print avg test loss\n",
    "    test_loss = test_loss / sum(class_total)\n",
    "    print(f'Test Loss: {test_loss:.6f}\\n')\n",
    "\n",
    "    for label in range(10):\n",
    "        print(\n",
    "            f'Test Accuracy of {label}: {int(100 * class_correct[label] / class_total[label])}% '\n",
    "            f'({int(np.sum(class_correct[label]))}/{int(np.sum(class_total[label]))})'\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        f'\\nTest Accuracy (Overall): {int(100 * np.sum(class_correct) / np.sum(class_total))}% ' \n",
    "        f'({int(np.sum(class_correct))}/{int(np.sum(class_total))})'\n",
    "    )\n",
    "\n",
    "\n",
    "# Load one element at a time\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=True)\n",
    "# required for encoding\n",
    "kernel_shape = model.conv1.kernel_size\n",
    "stride = model.conv1.stride[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encryption Parameters\n",
    "\n",
    "# controls precision of the fractional part\n",
    "bits_scale = 26\n",
    "\n",
    "# Create TenSEAL context\n",
    "context = ts.context(\n",
    "    ts.SCHEME_TYPE.CKKS,\n",
    "    poly_modulus_degree=8192,\n",
    "    coeff_mod_bit_sizes=[31, bits_scale, bits_scale, bits_scale, bits_scale, bits_scale, bits_scale, 31]\n",
    ")\n",
    "\n",
    "# set the scale\n",
    "context.global_scale = pow(2, bits_scale)\n",
    "\n",
    "# galois keys are required to do ciphertext rotations\n",
    "context.generate_galois_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n",
      "aaaaaaaaaaa\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m enc_model \u001b[38;5;241m=\u001b[39m EncConvNet(model)\n\u001b[0;32m----> 2\u001b[0m \u001b[43menc_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 62\u001b[0m, in \u001b[0;36menc_test\u001b[0;34m(context, model, test_loader, criterion, kernel_shape, stride)\u001b[0m\n\u001b[1;32m     57\u001b[0m x_enc, windows_nb \u001b[38;5;241m=\u001b[39m ts\u001b[38;5;241m.\u001b[39mim2col_encoding(\n\u001b[1;32m     58\u001b[0m     context, data\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist(), kernel_shape[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     59\u001b[0m     kernel_shape[\u001b[38;5;241m1\u001b[39m], stride\n\u001b[1;32m     60\u001b[0m )\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Encrypted evaluation\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m enc_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindows_nb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Decryption of result\u001b[39;00m\n\u001b[1;32m     64\u001b[0m output \u001b[38;5;241m=\u001b[39m enc_output\u001b[38;5;241m.\u001b[39mdecrypt()\n",
      "Cell \u001b[0;32mIn[3], line 46\u001b[0m, in \u001b[0;36mEncConvNet.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 38\u001b[0m, in \u001b[0;36mEncConvNet.forward\u001b[0;34m(self, enc_x, windows_nb)\u001b[0m\n\u001b[1;32m     36\u001b[0m enc_x\u001b[38;5;241m.\u001b[39msquare_()\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# fc1 layer\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m enc_x \u001b[38;5;241m=\u001b[39m \u001b[43menc_x\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1_weight\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1_bias\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# square activation\u001b[39;00m\n\u001b[1;32m     40\u001b[0m enc_x\u001b[38;5;241m.\u001b[39msquare_()\n",
      "File \u001b[0;32m~/Documents/Projects/SecML/secml2encryption/env/lib/python3.8/site-packages/tenseal/tensors/ckksvector.py:155\u001b[0m, in \u001b[0;36mCKKSVector.mm\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmm\u001b[39m(\u001b[38;5;28mself\u001b[39m, other) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCKKSVector\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    154\u001b[0m     other \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mm(other)\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "enc_model = EncConvNet(model)\n",
    "enc_test(context, enc_model, test_loader, criterion, kernel_shape, stride)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
